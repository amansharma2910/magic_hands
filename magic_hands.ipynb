{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magic Hands\n",
    "---\n",
    "\n",
    "In this project, we are going to work on our first Deep Learning project where we are going to train a Deep Neural Network to recognize your hand patterns. This can be used to create a simple game or control system where based on the detected pattern, you can execute a certain script that will perform some gesture-specific action.\n",
    "\n",
    "We are using the Tensorflow framework for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Project Dependencies\n",
    "---\n",
    "\n",
    "As the first step of this project, we are going to import all the necessary project dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform OS related operations\n",
    "import os\n",
    "\n",
    "# dependencies for model training and data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# dependencies for working with images\n",
    "import PIL\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "---\n",
    "\n",
    "Now, if you look at the images in the dataset, you will notice two things.\n",
    "* The dimensions for the images are different.\n",
    "* The images are rectangular in shape.\n",
    "\n",
    "Working with consistent data is easier than working with data that is inconsistent. We need to make all the items within our dataset consistent, because our model is going to expect an input of the same size and not varying size.\n",
    "\n",
    "So for the preprocessing, we are going to perform a center crop on all the images within the dataset, and make all the images of the same dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will perform the center crop on all the images to make them square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/amansharma2910/projects/magic_hands/data/0',\n",
       " '/Users/amansharma2910/projects/magic_hands/data/1',\n",
       " '/Users/amansharma2910/projects/magic_hands/data/2']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting data directories for different labels\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "label_dir = [os.path.join(data_dir, label) for label in os.listdir(data_dir) if not label.startswith('.')]\n",
    "label_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function to square center crop all the images in a directory\n",
    "\n",
    "def square_crop_images(dir: str) -> None:\n",
    "    images = [os.path.join(dir, image) for image in os.listdir(dir) if (image.endswith(\".jpg\") and not image.startswith(\".\"))]\n",
    "    for img in images:\n",
    "        im = PIL.Image.open(img)\n",
    "        \n",
    "        width, height = im.size\n",
    "        size = min(width, height)\n",
    "        \n",
    "        left = (width - size)/2\n",
    "        top = (height - size)/2\n",
    "        right = (width + size)/2\n",
    "        bottom = (height + size)/2\n",
    "\n",
    "        # crop the center of the image\n",
    "        im = im.crop((left, top, right, bottom))\n",
    "\n",
    "        # save image\n",
    "        im.save(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing center crop on the dataset images\n",
    "\n",
    "for dir in label_dir:\n",
    "    square_crop_images(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have square cropped the images. We need not resize them now. The images will be automatically resized as we load them into a Tensorflow compatible dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "---\n",
    "\n",
    "The next step is loading our dataset into a Tensorflow compatible dataset format, so we can finally begin training our deep learning model.\n",
    "\n",
    "For this, we are going to use the `image_dataset_from_directory` util method defined provided by Tensorflow. You can refer to the docs for the same here: https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 files belonging to 3 classes.\n",
      "Using 480 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  label_mode=\"int\",\n",
    "  seed=123,\n",
    "  image_size=(img_size, img_size),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 files belonging to 3 classes.\n",
      "Using 120 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  label_mode=\"int\",\n",
    "  seed=123,\n",
    "  image_size=(img_size, img_size),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the datasets. What this function basically does is, it converts each image into a tensor as it loads it, and associates a label with it. Let us verify that the dataset is working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 256, 256, 3)\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_ds:\n",
    "    images, labels = batch\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the class names respectively for the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, our images are loaded. Now let us visualize our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b82b339296dd701d8a244e650991aea6081e03987c3e8151a795b3d60ff9c77b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
